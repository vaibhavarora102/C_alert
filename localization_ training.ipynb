{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.2.0\n"
     ]
    }
   ],
   "source": [
    "import os, re, time, json\n",
    "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
    "import numpy as np\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot Utilities for Bounding Boxes [RUN ME]\n",
    "\n",
    "im_width = 224\n",
    "im_height = 224\n",
    "use_normalized_coordinates = True\n",
    "\n",
    "def draw_bounding_boxes_on_image_array(image,\n",
    "                                       boxes,\n",
    "                                       color=[],\n",
    "                                       thickness=1,\n",
    "                                       display_str_list=()):\n",
    "  \"\"\"Draws bounding boxes on image (numpy array).\n",
    "  Args:\n",
    "    image: a numpy array object.\n",
    "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
    "           The coordinates are in normalized format between [0, 1].\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list_list: a list of strings for each bounding box.\n",
    "  Raises:\n",
    "    ValueError: if boxes is not a [N, 4] array\n",
    "  \"\"\"\n",
    "  image_pil = PIL.Image.fromarray(image)\n",
    "  rgbimg = PIL.Image.new(\"RGBA\", image_pil.size)\n",
    "  rgbimg.paste(image_pil)\n",
    "  draw_bounding_boxes_on_image(rgbimg, boxes, color, thickness,\n",
    "                               display_str_list)\n",
    "  return np.array(rgbimg)\n",
    "  \n",
    "\n",
    "def draw_bounding_boxes_on_image(image,\n",
    "                                 boxes,\n",
    "                                 color=[],\n",
    "                                 thickness=1,\n",
    "                                 display_str_list=()):\n",
    "  \"\"\"Draws bounding boxes on image.\n",
    "  Args:\n",
    "    image: a PIL.Image object.\n",
    "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
    "           The coordinates are in normalized format between [0, 1].\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list: a list of strings for each bounding box.\n",
    "                           \n",
    "  Raises:\n",
    "    ValueError: if boxes is not a [N, 4] array\n",
    "  \"\"\"\n",
    "  boxes_shape = boxes.shape\n",
    "  if not boxes_shape:\n",
    "    return\n",
    "  if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n",
    "    raise ValueError('Input must be of size [N, 4]')\n",
    "  for i in range(boxes_shape[0]):\n",
    "    draw_bounding_box_on_image(image, boxes[i, 1], boxes[i, 0], boxes[i, 3],\n",
    "                               boxes[i, 2], color[i], thickness, display_str_list[i])\n",
    "        \n",
    "def draw_bounding_box_on_image(image,\n",
    "                               ymin,\n",
    "                               xmin,\n",
    "                               ymax,\n",
    "                               xmax,\n",
    "                               color='red',\n",
    "                               thickness=1,\n",
    "                               display_str=None,\n",
    "                               use_normalized_coordinates=True):\n",
    "  \"\"\"Adds a bounding box to an image.\n",
    "  Bounding box coordinates can be specified in either absolute (pixel) or\n",
    "  normalized coordinates by setting the use_normalized_coordinates argument.\n",
    "  Args:\n",
    "    image: a PIL.Image object.\n",
    "    ymin: ymin of bounding box.\n",
    "    xmin: xmin of bounding box.\n",
    "    ymax: ymax of bounding box.\n",
    "    xmax: xmax of bounding box.\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list: string to display in box\n",
    "    use_normalized_coordinates: If True (default), treat coordinates\n",
    "      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
    "      coordinates as absolute.\n",
    "  \"\"\"\n",
    "  draw = PIL.ImageDraw.Draw(image)\n",
    "  im_width, im_height = image.size\n",
    "  if use_normalized_coordinates:\n",
    "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                  ymin * im_height, ymax * im_height)\n",
    "  else:\n",
    "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
    "  draw.line([(left, top), (left, bottom), (right, bottom),\n",
    "             (right, top), (left, top)], width=thickness, fill=color)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Visualization Utilities [RUN ME]\n",
    "\"\"\"\n",
    "This cell contains helper functions used for visualization\n",
    "and downloads only. \n",
    "\n",
    "You can skip reading it, as there is very\n",
    "little Keras or Tensorflow related code here.\n",
    "\"\"\"\n",
    "\n",
    "# Matplotlib config\n",
    "plt.rc('image', cmap='gray')\n",
    "plt.rc('grid', linewidth=0)\n",
    "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
    "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
    "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
    "plt.rc('text', color='a8151a')\n",
    "plt.rc('figure', facecolor='F0F0F0')# Matplotlib fonts\n",
    "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
    "\n",
    "# pull a batch from the datasets. This code is not very nice, it gets much better in eager mode (TODO)\n",
    "def dataset_to_numpy_util(training_dataset, validation_dataset, N):\n",
    "  \n",
    "  # get one batch from each: 10000 validation digits, N training digits\n",
    "  batch_train_ds = training_dataset.unbatch().batch(N)\n",
    "  \n",
    "  # eager execution: loop through datasets normally\n",
    "  if tf.executing_eagerly():\n",
    "    for validation_digits, (validation_labels, validation_bboxes) in validation_dataset:\n",
    "      validation_digits = validation_digits.numpy()\n",
    "      validation_labels = validation_labels.numpy()\n",
    "      validation_bboxes = validation_bboxes.numpy()\n",
    "      break\n",
    "    for training_digits, (training_labels, training_bboxes) in batch_train_ds:\n",
    "      training_digits = training_digits.numpy()\n",
    "      training_labels = training_labels.numpy()\n",
    "      training_bboxes = training_bboxes.numpy()\n",
    "      break\n",
    "  \n",
    "  # these were one-hot encoded in the dataset\n",
    "  validation_labels = np.argmax(validation_labels, axis=1)\n",
    "  training_labels = np.argmax(training_labels, axis=1)\n",
    "  \n",
    "  return (training_digits, training_labels, training_bboxes,\n",
    "          validation_digits, validation_labels, validation_bboxes)\n",
    "\n",
    "# create digits from local fonts for testing\n",
    "def create_digits_from_local_fonts(n):\n",
    "  font_labels = []\n",
    "  img = PIL.Image.new('LA', (75*n, 75), color = (0,255)) # format 'LA': black in channel 0, alpha in channel 1\n",
    "  font1 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'DejaVuSansMono-Oblique.ttf'), 25)\n",
    "  font2 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'STIXGeneral.ttf'), 25)\n",
    "  d = PIL.ImageDraw.Draw(img)\n",
    "  for i in range(n):\n",
    "    font_labels.append(i%10)\n",
    "    d.text((7+i*75,0 if i<10 else -4), str(i%10), fill=(255,255), font=font1 if i<10 else font2)\n",
    "  font_digits = np.array(img.getdata(), np.float32)[:,0] / 255.0 # black in channel 0, alpha in channel 1 (discarded)\n",
    "  font_digits = np.reshape(np.stack(np.split(np.reshape(font_digits, [75, 75*n]), n, axis=1), axis=0), [n, 75*75])\n",
    "  return font_digits, font_labels\n",
    "\n",
    "\n",
    "# utility to display a row of digits with their predictions\n",
    "def display_digits_with_boxes(digits, predictions, labels, pred_bboxes, bboxes, iou, title):\n",
    "\n",
    "  n = 10\n",
    "\n",
    "  indexes = np.random.choice(len(predictions), size=n)\n",
    "  n_digits = digits[indexes]\n",
    "  n_predictions = predictions[indexes]\n",
    "  n_labels = labels[indexes]\n",
    "\n",
    "  n_iou = []\n",
    "  if len(iou) > 0:\n",
    "    n_iou = iou[indexes]\n",
    "\n",
    "  if (len(pred_bboxes) > 0):\n",
    "    n_pred_bboxes = pred_bboxes[indexes,:]\n",
    "\n",
    "  if (len(bboxes) > 0):\n",
    "    n_bboxes = bboxes[indexes,:]\n",
    "\n",
    "\n",
    "  n_digits = n_digits * 255.0\n",
    "  n_digits = n_digits.reshape(n, 75, 75)\n",
    "  fig = plt.figure(figsize=(20, 4))\n",
    "  plt.title(title)\n",
    "  plt.yticks([])\n",
    "  plt.xticks([])\n",
    "  \n",
    "  for i in range(10):\n",
    "    ax = fig.add_subplot(1, 10, i+1)\n",
    "    bboxes_to_plot = []\n",
    "    if (len(pred_bboxes) > i):\n",
    "      bboxes_to_plot.append(n_pred_bboxes[i])\n",
    "    \n",
    "    if (len(bboxes) > i):\n",
    "      bboxes_to_plot.append(n_bboxes[i])\n",
    "\n",
    "    img_to_draw = draw_bounding_boxes_on_image_array(image=n_digits[i], boxes=np.asarray(bboxes_to_plot), color=['red', 'green'], display_str_list=[\"true\", \"pred\"])\n",
    "    plt.xlabel(n_predictions[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    if n_predictions[i] != n_labels[i]:\n",
    "      ax.xaxis.label.set_color('red')\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.imshow(img_to_draw)\n",
    "\n",
    "    if len(iou) > i :\n",
    "      color = \"black\"\n",
    "      if (n_iou[i][0] < iou_threshold):\n",
    "        color = \"red\"\n",
    "      ax.text(0.2, -0.3, \"iou: %s\" %(n_iou[i][0]), color=color, transform=ax.transAxes)\n",
    "\n",
    "\n",
    "# utility to display training and validation curves\n",
    "def plot_metrics(metric_name, title, ylim=5):\n",
    "  plt.title(title)\n",
    "  plt.ylim(0,ylim)\n",
    "  plt.plot(history.history[metric_name],color='blue',label=metric_name)\n",
    "  plt.plot(history.history['val_' + metric_name],color='green',label='val_' + metric_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU\n",
      "Number of accelerators:  1\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
    "print('Running on CPU')\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64 * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, MaxPooling2D, Flatten, Activation, Conv2D,AveragePooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "src =\"/home/vaibhav/Documents/ku_hack/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1936 images belonging to 2 classes.\n",
      "Found 484 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                  shear_range=0.2,\n",
    "                                  zoom_range=0.2,\n",
    "                                  horizontal_flip=True,\n",
    "                                  vertical_flip=True,\n",
    "                                  validation_split=0.2)\n",
    "\n",
    "# ------------------------------------------------------validation_split=0.2\n",
    "img_size = 224\n",
    "training_dataset = datagen.flow_from_directory(src,\n",
    "                                           target_size=(img_size,img_size),\n",
    "                                           batch_size=16,\n",
    "                                           class_mode='binary',\n",
    "                                           subset='training')\n",
    "\n",
    "validation_dataset = datagen.flow_from_directory(src,\n",
    "                                           target_size=(img_size,img_size),\n",
    "                                           batch_size=16,\n",
    "                                           class_mode='binary',\n",
    "                                           subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 222, 222, 16) 448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 111, 111, 16) 0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 109, 109, 32) 4640        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 54, 54, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 52, 52, 64)   18496       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 26, 26, 64)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 43264)        0           average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          5537920     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "classification (Dense)          (None, 1)            129         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bounding_box (Dense)            (None, 4)            516         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 5,562,149\n",
      "Trainable params: 5,562,149\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Feature extractor is the CNN that is made up of convolution and pooling layers.\n",
    "'''\n",
    "def feature_extractor(inputs):\n",
    "    x = tf.keras.layers.Conv2D(16, activation='relu', kernel_size=3, input_shape=(224, 224, 3))(inputs)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32,kernel_size=3,activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64,kernel_size=3,activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((2, 2))(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "'''\n",
    "dense_layers adds a flatten and dense layer.\n",
    "This will follow the feature extraction layers\n",
    "'''\n",
    "def dense_layers(inputs):\n",
    "  x = tf.keras.layers.Flatten()(inputs)\n",
    "  x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "  return x\n",
    "\n",
    "\n",
    "'''\n",
    "Classifier defines the classification output.\n",
    "This has a set of fully connected layers and a softmax layer.\n",
    "'''\n",
    "def classifier(inputs):\n",
    "\n",
    "  classification_output = tf.keras.layers.Dense(1, activation='sigmoid', name = 'classification')(inputs)\n",
    "  return classification_output\n",
    "\n",
    "\n",
    "def bounding_box_regression(inputs):\n",
    "    bounding_box_regression_output = tf.keras.layers.Dense(units = '4', name = 'bounding_box')(inputs)\n",
    "    return bounding_box_regression_output\n",
    "\n",
    "\n",
    "def final_model(inputs):\n",
    "    feature_cnn = feature_extractor(inputs)\n",
    "    dense_output = dense_layers(feature_cnn)\n",
    "\n",
    "    '''\n",
    "    The model branches here.  \n",
    "    The dense layer's output gets fed into two branches:\n",
    "    classification_output and bounding_box_output\n",
    "    '''\n",
    "    classification_output = classifier(dense_output)\n",
    "    bounding_box_output = bounding_box_regression(dense_output)\n",
    "\n",
    "    model = tf.keras.Model(inputs = inputs, outputs = [classification_output, bounding_box_output])\n",
    "\n",
    "    return model\n",
    "  \n",
    "\n",
    "def define_and_compile_model(inputs):\n",
    "  model = final_model(inputs)\n",
    "  \n",
    "  model.compile(optimizer='adam', \n",
    "              loss = {'classification' : 'binary_crossentropy',\n",
    "                      'bounding_box' : 'mse'\n",
    "                     },\n",
    "              metrics = {'classification' : 'accuracy',\n",
    "                         'bounding_box' : 'mse'\n",
    "                        })\n",
    "  return model\n",
    "\n",
    "    \n",
    "with strategy.scope():\n",
    "  inputs = tf.keras.layers.Input(shape=(224, 224, 3,))\n",
    "  model = define_and_compile_model(inputs)\n",
    "\n",
    "# print model layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint  = ModelCheckpoint(\"./model/crime_alert.h5\",\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             verbose = 2)\n",
    "\n",
    "earlystop =EarlyStopping(monitor='val_loss',\n",
    "                        min_delta=0,\n",
    "                        patience=10,\n",
    "                        verbose =2,\n",
    "                        restore_best_weights=True)\n",
    "callbacks= [checkpoint,earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-2fc91dffe441>:5: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/15\n",
      "121/121 [==============================] - ETA: 0s - loss: 1.0241 - classification_loss: 0.6610 - bounding_box_loss: 0.3631 - classification_accuracy: 0.6276 - bounding_box_mse: 0.3631\n",
      "Epoch 00001: val_loss improved from inf to 0.85265, saving model to ./model/crime_alert.h5\n",
      "121/121 [==============================] - 71s 590ms/step - loss: 1.0241 - classification_loss: 0.6610 - bounding_box_loss: 0.3631 - classification_accuracy: 0.6276 - bounding_box_mse: 0.3631 - val_loss: 0.8527 - val_classification_loss: 0.6373 - val_bounding_box_loss: 0.2153 - val_classification_accuracy: 0.5312 - val_bounding_box_mse: 0.2153\n",
      "Epoch 2/15\n",
      "121/121 [==============================] - ETA: 0s - loss: 0.8675 - classification_loss: 0.6237 - bounding_box_loss: 0.2438 - classification_accuracy: 0.6596 - bounding_box_mse: 0.2438\n",
      "Epoch 00002: val_loss improved from 0.85265 to 0.75318, saving model to ./model/crime_alert.h5\n",
      "121/121 [==============================] - 82s 674ms/step - loss: 0.8675 - classification_loss: 0.6237 - bounding_box_loss: 0.2438 - classification_accuracy: 0.6596 - bounding_box_mse: 0.2438 - val_loss: 0.7532 - val_classification_loss: 0.5442 - val_bounding_box_loss: 0.2089 - val_classification_accuracy: 0.7188 - val_bounding_box_mse: 0.2089\n",
      "Epoch 3/15\n",
      "121/121 [==============================] - ETA: 0s - loss: 0.8141 - classification_loss: 0.5962 - bounding_box_loss: 0.2180 - classification_accuracy: 0.7014 - bounding_box_mse: 0.2180\n",
      "Epoch 00003: val_loss did not improve from 0.75318\n",
      "121/121 [==============================] - 73s 600ms/step - loss: 0.8141 - classification_loss: 0.5962 - bounding_box_loss: 0.2180 - classification_accuracy: 0.7014 - bounding_box_mse: 0.2180 - val_loss: 0.8626 - val_classification_loss: 0.6276 - val_bounding_box_loss: 0.2350 - val_classification_accuracy: 0.6562 - val_bounding_box_mse: 0.2350\n",
      "Epoch 4/15\n",
      "121/121 [==============================] - ETA: 0s - loss: 0.7972 - classification_loss: 0.5807 - bounding_box_loss: 0.2165 - classification_accuracy: 0.7180 - bounding_box_mse: 0.2165\n",
      "Epoch 00004: val_loss did not improve from 0.75318\n",
      "121/121 [==============================] - 73s 601ms/step - loss: 0.7972 - classification_loss: 0.5807 - bounding_box_loss: 0.2165 - classification_accuracy: 0.7180 - bounding_box_mse: 0.2165 - val_loss: 0.7646 - val_classification_loss: 0.5486 - val_bounding_box_loss: 0.2159 - val_classification_accuracy: 0.6562 - val_bounding_box_mse: 0.2159\n",
      "Epoch 5/15\n",
      "121/121 [==============================] - ETA: 0s - loss: 0.7521 - classification_loss: 0.5532 - bounding_box_loss: 0.1989 - classification_accuracy: 0.7278 - bounding_box_mse: 0.1989\n",
      "Epoch 00005: val_loss improved from 0.75318 to 0.66537, saving model to ./model/crime_alert.h5\n",
      "121/121 [==============================] - 73s 602ms/step - loss: 0.7521 - classification_loss: 0.5532 - bounding_box_loss: 0.1989 - classification_accuracy: 0.7278 - bounding_box_mse: 0.1989 - val_loss: 0.6654 - val_classification_loss: 0.4905 - val_bounding_box_loss: 0.1749 - val_classification_accuracy: 0.7500 - val_bounding_box_mse: 0.1749\n",
      "Epoch 6/15\n",
      "121/121 [==============================] - ETA: 0s - loss: 0.7301 - classification_loss: 0.5389 - bounding_box_loss: 0.1912 - classification_accuracy: 0.7433 - bounding_box_mse: 0.1912\n",
      "Epoch 00006: val_loss did not improve from 0.66537\n",
      "121/121 [==============================] - 73s 603ms/step - loss: 0.7301 - classification_loss: 0.5389 - bounding_box_loss: 0.1912 - classification_accuracy: 0.7433 - bounding_box_mse: 0.1912 - val_loss: 0.7492 - val_classification_loss: 0.5335 - val_bounding_box_loss: 0.2157 - val_classification_accuracy: 0.7500 - val_bounding_box_mse: 0.2157\n",
      "Epoch 7/15\n",
      "121/121 [==============================] - ETA: 0s - loss: 0.7102 - classification_loss: 0.5253 - bounding_box_loss: 0.1849 - classification_accuracy: 0.7397 - bounding_box_mse: 0.1849\n",
      "Epoch 00007: val_loss did not improve from 0.66537\n",
      "121/121 [==============================] - 70s 581ms/step - loss: 0.7102 - classification_loss: 0.5253 - bounding_box_loss: 0.1849 - classification_accuracy: 0.7397 - bounding_box_mse: 0.1849 - val_loss: 0.7925 - val_classification_loss: 0.5924 - val_bounding_box_loss: 0.2001 - val_classification_accuracy: 0.6562 - val_bounding_box_mse: 0.2001\n",
      "Epoch 8/15\n",
      "121/121 [==============================] - ETA: 0s - loss: 0.6923 - classification_loss: 0.5101 - bounding_box_loss: 0.1822 - classification_accuracy: 0.7541 - bounding_box_mse: 0.1822\n",
      "Epoch 00008: val_loss did not improve from 0.66537\n",
      "121/121 [==============================] - 70s 581ms/step - loss: 0.6923 - classification_loss: 0.5101 - bounding_box_loss: 0.1822 - classification_accuracy: 0.7541 - bounding_box_mse: 0.1822 - val_loss: 0.9453 - val_classification_loss: 0.6891 - val_bounding_box_loss: 0.2562 - val_classification_accuracy: 0.6562 - val_bounding_box_mse: 0.2562\n",
      "Epoch 9/15\n",
      "121/121 [==============================] - ETA: 0s - loss: 0.6693 - classification_loss: 0.4937 - bounding_box_loss: 0.1756 - classification_accuracy: 0.7634 - bounding_box_mse: 0.1756\n",
      "Epoch 00009: val_loss improved from 0.66537 to 0.61194, saving model to ./model/crime_alert.h5\n",
      "121/121 [==============================] - 71s 587ms/step - loss: 0.6693 - classification_loss: 0.4937 - bounding_box_loss: 0.1756 - classification_accuracy: 0.7634 - bounding_box_mse: 0.1756 - val_loss: 0.6119 - val_classification_loss: 0.4466 - val_bounding_box_loss: 0.1653 - val_classification_accuracy: 0.7500 - val_bounding_box_mse: 0.1653\n",
      "Epoch 10/15\n",
      "121/121 [==============================] - ETA: 0s - loss: 0.6630 - classification_loss: 0.4899 - bounding_box_loss: 0.1731 - classification_accuracy: 0.7686 - bounding_box_mse: 0.1731\n",
      "Epoch 00010: val_loss did not improve from 0.61194\n",
      "121/121 [==============================] - 70s 577ms/step - loss: 0.6630 - classification_loss: 0.4899 - bounding_box_loss: 0.1731 - classification_accuracy: 0.7686 - bounding_box_mse: 0.1731 - val_loss: 0.7745 - val_classification_loss: 0.5850 - val_bounding_box_loss: 0.1895 - val_classification_accuracy: 0.7812 - val_bounding_box_mse: 0.1895\n",
      "Epoch 11/15\n",
      "121/121 [==============================] - ETA: 0s - loss: 0.6592 - classification_loss: 0.4825 - bounding_box_loss: 0.1767 - classification_accuracy: 0.7800 - bounding_box_mse: 0.1767\n",
      "Epoch 00011: val_loss did not improve from 0.61194\n",
      "121/121 [==============================] - 70s 581ms/step - loss: 0.6592 - classification_loss: 0.4825 - bounding_box_loss: 0.1767 - classification_accuracy: 0.7800 - bounding_box_mse: 0.1767 - val_loss: 0.7853 - val_classification_loss: 0.5792 - val_bounding_box_loss: 0.2061 - val_classification_accuracy: 0.6875 - val_bounding_box_mse: 0.2061\n",
      "Epoch 12/15\n",
      "121/121 [==============================] - ETA: 0s - loss: 0.6540 - classification_loss: 0.4842 - bounding_box_loss: 0.1699 - classification_accuracy: 0.7707 - bounding_box_mse: 0.1699\n",
      "Epoch 00012: val_loss did not improve from 0.61194\n",
      "121/121 [==============================] - 70s 580ms/step - loss: 0.6540 - classification_loss: 0.4842 - bounding_box_loss: 0.1699 - classification_accuracy: 0.7707 - bounding_box_mse: 0.1699 - val_loss: 0.6298 - val_classification_loss: 0.4423 - val_bounding_box_loss: 0.1875 - val_classification_accuracy: 0.7812 - val_bounding_box_mse: 0.1875\n",
      "Epoch 13/15\n",
      "121/121 [==============================] - ETA: 0s - loss: 0.6347 - classification_loss: 0.4684 - bounding_box_loss: 0.1662 - classification_accuracy: 0.7789 - bounding_box_mse: 0.1662\n",
      "Epoch 00013: val_loss did not improve from 0.61194\n",
      "121/121 [==============================] - 70s 579ms/step - loss: 0.6347 - classification_loss: 0.4684 - bounding_box_loss: 0.1662 - classification_accuracy: 0.7789 - bounding_box_mse: 0.1662 - val_loss: 0.6340 - val_classification_loss: 0.4296 - val_bounding_box_loss: 0.2044 - val_classification_accuracy: 0.8750 - val_bounding_box_mse: 0.2044\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121/121 [==============================] - ETA: 0s - loss: 0.6119 - classification_loss: 0.4496 - bounding_box_loss: 0.1623 - classification_accuracy: 0.7867 - bounding_box_mse: 0.1623\n",
      "Epoch 00014: val_loss did not improve from 0.61194\n",
      "121/121 [==============================] - 70s 577ms/step - loss: 0.6119 - classification_loss: 0.4496 - bounding_box_loss: 0.1623 - classification_accuracy: 0.7867 - bounding_box_mse: 0.1623 - val_loss: 0.6572 - val_classification_loss: 0.4811 - val_bounding_box_loss: 0.1761 - val_classification_accuracy: 0.7500 - val_bounding_box_mse: 0.1761\n",
      "Epoch 15/15\n",
      "121/121 [==============================] - ETA: 0s - loss: 0.5970 - classification_loss: 0.4404 - bounding_box_loss: 0.1566 - classification_accuracy: 0.8006 - bounding_box_mse: 0.1566\n",
      "Epoch 00015: val_loss improved from 0.61194 to 0.40863, saving model to ./model/crime_alert.h5\n",
      "121/121 [==============================] - 71s 588ms/step - loss: 0.5970 - classification_loss: 0.4404 - bounding_box_loss: 0.1566 - classification_accuracy: 0.8006 - bounding_box_mse: 0.1566 - val_loss: 0.4086 - val_classification_loss: 0.3004 - val_bounding_box_loss: 0.1082 - val_classification_accuracy: 0.8750 - val_bounding_box_mse: 0.1082\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4937 - classification_loss: 0.3742 - bounding_box_loss: 0.1195 - classification_accuracy: 0.8125 - bounding_box_mse: 0.1195\n",
      "Validation accuracy:  0.8125\n"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(training_dataset,validation_data=validation_dataset,\n",
    "                                 epochs=15,\n",
    "                                 steps_per_epoch=training_dataset.samples//16,\n",
    "                                 validation_steps=2,\n",
    "                                 callbacks=callbacks)\n",
    "\n",
    "loss, classification_loss, bounding_box_loss, classification_accuracy, bounding_box_mse = model.evaluate(validation_dataset, steps=1)\n",
    "print(\"Validation accuracy: \", classification_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
